{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatandDogPicturesClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5X2ootLUK0"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfjXeyx8Lh-1"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "import keras_preprocessing\r\n",
        "from keras_preprocessing import image\r\n",
        "from tensorflow.keras import layers\r\n",
        "from keras_preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input\r\n",
        "from tensorflow.keras.models import Model \r\n",
        "from tensorflow.keras.metrics import Precision\r\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw5BpINBL6T-"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs={}):\r\n",
        "    if((logs.get('val_accuracy')>=0.9700) and (logs.get('val_precision')>=0.9775) and (logs.get('val_mae')<=0.0385)):\r\n",
        "      print(\"\\nTa-da! ₍ᐢ•ﻌ•ᐢ₎*･ﾟ｡ Reached 97% accuracy, 97.75% precision, and less than .0385 MAE, so cancelling training!\")\r\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbyIb0rL7S1"
      },
      "source": [
        "callbacks = myCallback()\r\n",
        "precision = Precision()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSvl4ETLk7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec54a678-ecae-4c46-b596-fdad60da12b4"
      },
      "source": [
        "#This block gets the pre-trained model\r\n",
        "\r\n",
        "!wget --no-check-certificate \\\r\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\r\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\r\n",
        "  \r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\n",
        "\r\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\r\n",
        "\r\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \r\n",
        "                                include_top = False, \r\n",
        "                                weights = None)\r\n",
        "\r\n",
        "pre_trained_model.load_weights(local_weights_file)\r\n",
        "\r\n",
        "for layer in pre_trained_model.layers:\r\n",
        "  layer.trainable = False\r\n",
        "  \r\n",
        "# pre_trained_model.summary()\r\n",
        "\r\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\r\n",
        "print('last layer output shape: ', last_layer.output_shape)\r\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 20:04:29--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.31.128, 172.217.164.176, 172.217.9.208, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.31.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M   197MB/s    in 0.4s    \n",
            "\n",
            "2021-01-31 20:04:29 (197 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOl1BDmLrHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767c8b72-af1e-443e-9150-26372289c7fe"
      },
      "source": [
        "!wget --no-check-certificate \\\r\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\r\n",
        "       -O /tmp/cats_and_dogs_filtered.zip\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "local_zip = '//tmp/cats_and_dogs_filtered.zip'\r\n",
        "\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "\r\n",
        "zip_ref.extractall('/tmp')\r\n",
        "zip_ref.close()\r\n",
        "\r\n",
        "# Define our example directories and files\r\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\r\n",
        "\r\n",
        "train_dir = os.path.join( base_dir, 'train')\r\n",
        "validation_dir = os.path.join( base_dir, 'validation')\r\n",
        "\r\n",
        "\r\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') # Directory with our training cat pictures\r\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs') # Directory with our training dog pictures\r\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats') # Directory with our validation cat pictures\r\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')# Directory with our validation dog pictures\r\n",
        "\r\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\r\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\r\n",
        "\r\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\r\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\r\n",
        "                                   rotation_range = 40,\r\n",
        "                                   width_shift_range = 0.2,\r\n",
        "                                   height_shift_range = 0.2,\r\n",
        "                                   shear_range = 0.2,\r\n",
        "                                   zoom_range = 0.2,\r\n",
        "                                   horizontal_flip = True)\r\n",
        "\r\n",
        "# Note that the validation data should not be augmented!\r\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\r\n",
        "\r\n",
        "# Flow training images in batches of 20 using train_datagen generator\r\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\r\n",
        "                                                    batch_size = 20,\r\n",
        "                                                    class_mode = 'binary', \r\n",
        "                                                    target_size = (150, 150))     \r\n",
        "\r\n",
        "# Flow validation images in batches of 20 using test_datagen generator\r\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\r\n",
        "                                                          batch_size  = 20,\r\n",
        "                                                          class_mode  = 'binary', \r\n",
        "                                                          target_size = (150, 150))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 20:04:31--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.164.176, 172.217.2.112, 172.217.164.144, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.164.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   170MB/s    in 0.4s    \n",
            "\n",
            "2021-01-31 20:04:32 (170 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3OftGS5LyB8"
      },
      "source": [
        "# Flatten the output layer to 1 dimension\r\n",
        "x = Flatten()(last_output)\r\n",
        "x = Dense(644, activation='swish')(x)\r\n",
        "x = Dropout(0.33)(x)\r\n",
        "x = Dense(644, activation='swish')(x)\r\n",
        "x = Dropout(0.33)(x) \r\n",
        "x = Dense(124, activation='swish')(x) \r\n",
        "x = Dropout(0.33)(x)    \r\n",
        "x = Dense(62, activation='swish')(x) \r\n",
        "x = Dropout(0.33)(x)               \r\n",
        "x = Dense(1, activation='sigmoid')(x) \r\n",
        "\r\n",
        "model1 = Model(pre_trained_model.input, x) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvILwlUnMP7j"
      },
      "source": [
        "#next, create a custom optimizer\r\n",
        "optimizer1 = RMSprop(lr=0.0001)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhNTJPoMQoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f036ac2-dac9-4ef1-8dc1-f591f602db2c"
      },
      "source": [
        "model1.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=optimizer1,\r\n",
        "              metrics=['mae','accuracy', precision])\r\n",
        "\r\n",
        "history1 = model1.fit(train_generator, \r\n",
        "                      validation_data = validation_generator,\r\n",
        "                      epochs=40, \r\n",
        "                      steps_per_epoch = 100,\r\n",
        "                      validation_steps = 50,\r\n",
        "                      verbose = 2,\r\n",
        "                       callbacks=[callbacks])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "100/100 - 28s - loss: 0.3427 - mae: 0.2260 - accuracy: 0.8390 - precision: 0.8466 - val_loss: 0.1116 - val_mae: 0.0629 - val_accuracy: 0.9530 - val_precision: 0.9415\n",
            "Epoch 2/40\n",
            "100/100 - 17s - loss: 0.2263 - mae: 0.1246 - accuracy: 0.9050 - precision: 0.9108 - val_loss: 0.0993 - val_mae: 0.0526 - val_accuracy: 0.9620 - val_precision: 0.9695\n",
            "Epoch 3/40\n",
            "100/100 - 17s - loss: 0.1910 - mae: 0.1014 - accuracy: 0.9305 - precision: 0.9362 - val_loss: 0.1065 - val_mae: 0.0405 - val_accuracy: 0.9660 - val_precision: 0.9679\n",
            "Epoch 4/40\n",
            "100/100 - 17s - loss: 0.1947 - mae: 0.0999 - accuracy: 0.9265 - precision: 0.9356 - val_loss: 0.1584 - val_mae: 0.0600 - val_accuracy: 0.9430 - val_precision: 0.9911\n",
            "Epoch 5/40\n",
            "100/100 - 17s - loss: 0.1923 - mae: 0.0906 - accuracy: 0.9265 - precision: 0.9312 - val_loss: 0.0821 - val_mae: 0.0423 - val_accuracy: 0.9690 - val_precision: 0.9757\n",
            "Epoch 6/40\n",
            "100/100 - 17s - loss: 0.1704 - mae: 0.0838 - accuracy: 0.9365 - precision: 0.9387 - val_loss: 0.0844 - val_mae: 0.0405 - val_accuracy: 0.9660 - val_precision: 0.9814\n",
            "Epoch 7/40\n",
            "100/100 - 17s - loss: 0.1741 - mae: 0.0864 - accuracy: 0.9320 - precision: 0.9399 - val_loss: 0.0869 - val_mae: 0.0403 - val_accuracy: 0.9680 - val_precision: 0.9643\n",
            "Epoch 8/40\n",
            "100/100 - 17s - loss: 0.1659 - mae: 0.0796 - accuracy: 0.9400 - precision: 0.9435 - val_loss: 0.1006 - val_mae: 0.0427 - val_accuracy: 0.9620 - val_precision: 0.9833\n",
            "Epoch 9/40\n",
            "100/100 - 17s - loss: 0.1681 - mae: 0.0797 - accuracy: 0.9380 - precision: 0.9497 - val_loss: 0.0981 - val_mae: 0.0439 - val_accuracy: 0.9670 - val_precision: 0.9569\n",
            "Epoch 10/40\n",
            "100/100 - 17s - loss: 0.1461 - mae: 0.0723 - accuracy: 0.9470 - precision: 0.9515 - val_loss: 0.1004 - val_mae: 0.0428 - val_accuracy: 0.9670 - val_precision: 0.9642\n",
            "Epoch 11/40\n",
            "100/100 - 17s - loss: 0.1594 - mae: 0.0693 - accuracy: 0.9450 - precision: 0.9477 - val_loss: 0.1125 - val_mae: 0.0437 - val_accuracy: 0.9660 - val_precision: 0.9569\n",
            "Epoch 12/40\n",
            "100/100 - 17s - loss: 0.1734 - mae: 0.0786 - accuracy: 0.9385 - precision: 0.9461 - val_loss: 0.0932 - val_mae: 0.0386 - val_accuracy: 0.9680 - val_precision: 0.9776\n",
            "Epoch 13/40\n",
            "100/100 - 17s - loss: 0.1542 - mae: 0.0662 - accuracy: 0.9480 - precision: 0.9553 - val_loss: 0.1040 - val_mae: 0.0381 - val_accuracy: 0.9700 - val_precision: 0.9835\n",
            "\n",
            "Ta-da! ₍ᐢ•ﻌ•ᐢ₎*･ﾟ｡ Reached 97% accuracy, 97.75% precision, and less than .0385 MAE, so cancelling training!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}