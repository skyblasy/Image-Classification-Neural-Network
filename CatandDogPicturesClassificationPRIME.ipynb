{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatandDogPicturesClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH5X2ootLUK0"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfjXeyx8Lh-1"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from tensorflow.keras import layers\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Model \n",
        "from tensorflow.keras.metrics import Precision\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw5BpINBL6T-"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if((logs.get('val_accuracy')>=0.9700) and (logs.get('val_precision')>=0.9775) and (logs.get('val_mae')<=0.0385)):\n",
        "      print(\"\\nTa-da! ₍ᐢ•ﻌ•ᐢ₎*･ﾟ｡ Reached 97% accuracy, 97.75% precision, and less than .0385 MAE, so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLbyIb0rL7S1"
      },
      "source": [
        "callbacks = myCallback()\n",
        "precision = Precision()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PSvl4ETLk7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6592d3-1729-4a52-cebd-46324c44e0ad"
      },
      "source": [
        "#This block gets the pre-trained model\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-28 02:04:52--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.203.128, 64.233.187.128, 64.233.188.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.203.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M  64.9MB/s    in 1.3s    \n",
            "\n",
            "2021-09-28 02:04:54 (64.9 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOl1BDmLrHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df81fb99-b7c1-4616-d055-b6cf3d4c56e4"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "       -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '//tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Define our example directories and files\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir = os.path.join( base_dir, 'train')\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, 'cats') # Directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs') # Directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats') # Directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')# Directory with our validation dog pictures\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "\n",
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = 20,\n",
        "                                                    class_mode = 'binary', \n",
        "                                                    target_size = (150, 150))     \n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = 20,\n",
        "                                                          class_mode  = 'binary', \n",
        "                                                          target_size = (150, 150))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-28 02:04:56--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.8.128, 74.125.203.128, 74.125.204.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.8.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  55.4MB/s    in 1.2s    \n",
            "\n",
            "2021-09-28 02:04:58 (55.4 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3OftGS5LyB8"
      },
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = Flatten()(last_output)\n",
        "x = Dense(644, activation='swish')(x)\n",
        "x = Dropout(0.33)(x)\n",
        "x = Dense(644, activation='swish')(x)\n",
        "x = Dropout(0.33)(x) \n",
        "x = Dense(124, activation='swish')(x) \n",
        "x = Dropout(0.33)(x)    \n",
        "x = Dense(62, activation='swish')(x) \n",
        "x = Dropout(0.33)(x)               \n",
        "x = Dense(1, activation='sigmoid')(x) \n",
        "\n",
        "model1 = Model(pre_trained_model.input, x) "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvILwlUnMP7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e3c28a-75da-4ac1-bb54-a3a2612539e1"
      },
      "source": [
        "#next, create a custom optimizer\n",
        "optimizer1 = RMSprop(lr=0.0001)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhNTJPoMQoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0286802a-0dc0-46ed-9a67-eb995dd25ba2"
      },
      "source": [
        "model1.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer1,\n",
        "              metrics=['mae','accuracy', precision])\n",
        "\n",
        "history1 = model1.fit(train_generator, \n",
        "                      validation_data = validation_generator,\n",
        "                      epochs=40, \n",
        "                      steps_per_epoch = 100,\n",
        "                      validation_steps = 50,\n",
        "                      verbose = 2,\n",
        "                       callbacks=[callbacks])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "100/100 - 59s - loss: 0.3411 - mae: 0.2167 - accuracy: 0.8485 - precision: 0.8611 - val_loss: 0.1219 - val_mae: 0.0726 - val_accuracy: 0.9500 - val_precision: 0.9198\n",
            "Epoch 2/40\n",
            "100/100 - 22s - loss: 0.2164 - mae: 0.1221 - accuracy: 0.9170 - precision: 0.9246 - val_loss: 0.1169 - val_mae: 0.0552 - val_accuracy: 0.9620 - val_precision: 0.9425\n",
            "Epoch 3/40\n",
            "100/100 - 23s - loss: 0.2009 - mae: 0.1072 - accuracy: 0.9145 - precision: 0.9191 - val_loss: 0.1084 - val_mae: 0.0546 - val_accuracy: 0.9530 - val_precision: 0.9347\n",
            "Epoch 4/40\n",
            "100/100 - 22s - loss: 0.1730 - mae: 0.0905 - accuracy: 0.9325 - precision: 0.9364 - val_loss: 0.1740 - val_mae: 0.0627 - val_accuracy: 0.9430 - val_precision: 0.9064\n",
            "Epoch 5/40\n",
            "100/100 - 23s - loss: 0.1860 - mae: 0.0879 - accuracy: 0.9330 - precision: 0.9304 - val_loss: 0.1135 - val_mae: 0.0459 - val_accuracy: 0.9580 - val_precision: 0.9404\n",
            "Epoch 6/40\n",
            "100/100 - 22s - loss: 0.1775 - mae: 0.0851 - accuracy: 0.9340 - precision: 0.9393 - val_loss: 0.0902 - val_mae: 0.0463 - val_accuracy: 0.9700 - val_precision: 0.9626\n",
            "Epoch 7/40\n",
            "100/100 - 22s - loss: 0.1658 - mae: 0.0798 - accuracy: 0.9405 - precision: 0.9472 - val_loss: 0.0978 - val_mae: 0.0406 - val_accuracy: 0.9650 - val_precision: 0.9604\n",
            "Epoch 8/40\n",
            "100/100 - 22s - loss: 0.1571 - mae: 0.0748 - accuracy: 0.9400 - precision: 0.9427 - val_loss: 0.1253 - val_mae: 0.0529 - val_accuracy: 0.9520 - val_precision: 0.9264\n",
            "Epoch 9/40\n",
            "100/100 - 22s - loss: 0.1650 - mae: 0.0794 - accuracy: 0.9390 - precision: 0.9443 - val_loss: 0.1163 - val_mae: 0.0453 - val_accuracy: 0.9580 - val_precision: 0.9404\n",
            "Epoch 10/40\n",
            "100/100 - 23s - loss: 0.1728 - mae: 0.0806 - accuracy: 0.9355 - precision: 0.9351 - val_loss: 0.0930 - val_mae: 0.0420 - val_accuracy: 0.9640 - val_precision: 0.9567\n",
            "Epoch 11/40\n",
            "100/100 - 22s - loss: 0.1617 - mae: 0.0777 - accuracy: 0.9360 - precision: 0.9431 - val_loss: 0.0920 - val_mae: 0.0397 - val_accuracy: 0.9670 - val_precision: 0.9661\n",
            "Epoch 12/40\n",
            "100/100 - 23s - loss: 0.1735 - mae: 0.0762 - accuracy: 0.9420 - precision: 0.9465 - val_loss: 0.0778 - val_mae: 0.0440 - val_accuracy: 0.9630 - val_precision: 0.9530\n",
            "Epoch 13/40\n",
            "100/100 - 22s - loss: 0.1563 - mae: 0.0750 - accuracy: 0.9400 - precision: 0.9472 - val_loss: 0.1253 - val_mae: 0.0525 - val_accuracy: 0.9530 - val_precision: 0.9266\n",
            "Epoch 14/40\n",
            "100/100 - 22s - loss: 0.1612 - mae: 0.0692 - accuracy: 0.9460 - precision: 0.9416 - val_loss: 0.1072 - val_mae: 0.0467 - val_accuracy: 0.9600 - val_precision: 0.9389\n",
            "Epoch 15/40\n",
            "100/100 - 21s - loss: 0.1432 - mae: 0.0643 - accuracy: 0.9535 - precision: 0.9567 - val_loss: 0.0896 - val_mae: 0.0347 - val_accuracy: 0.9700 - val_precision: 0.9757\n",
            "Epoch 16/40\n",
            "100/100 - 21s - loss: 0.1487 - mae: 0.0670 - accuracy: 0.9465 - precision: 0.9496 - val_loss: 0.0756 - val_mae: 0.0375 - val_accuracy: 0.9720 - val_precision: 0.9816\n",
            "\n",
            "Ta-da! ₍ᐢ•ﻌ•ᐢ₎*･ﾟ｡ Reached 97% accuracy, 97.75% precision, and less than .0385 MAE, so cancelling training!\n"
          ]
        }
      ]
    }
  ]
}